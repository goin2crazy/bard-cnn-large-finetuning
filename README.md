# BART-Large-CNN Fine-Tuning with Transformers

This repository contains a Jupyter Notebook (`*.ipynb`) that demonstrates different fine-tuning methods for the 'bart-large-cnn' model using the Transformers library. The notebook explores full fine-tuning, fine-tuning only the last layers of the encoder and decoder, and LORA fine-tuning with various datasets, including preprocessed and raw datasets.

## Instructions

### Environment Setup

Before running the notebook, ensure you have the required libraries installed. You can install them using the following steps:

1. Open the `todo.txt` file.
2. Follow the instructions to install the necessary libraries using pip or conda.

### Running the Notebook

1. Open the `*.ipynb` file in Jupyter Notebook or JupyterLab.
2. Follow the instructions within the notebook to run different fine-tuning experiments with the 'bart-large-cnn' model.

## Contents

- `*.ipynb`: Jupyter Notebook with code for fine-tuning 'bart-large-cnn' using Transformers.
- `todo.txt`: Instructions for installing required libraries.
- Additional files for datasets and model configurations.

## Acknowledgements

- This project uses the Transformers library by Hugging Face: [full or last layers finetuning](https://huggingface.co/doublecringe123/bardt-large-cnn-dialoguesum-booksum), [lora finetuning](https://huggingface.co/doublecringe123/bardt-large-cnn-dialoguesum-booksum-lora)
- The 'bart-large-cnn' model is provided by Facebook AI Research.
